{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNojxO5FHDDPUwM8TsVFMm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C-CIGayXCXp"
      },
      "outputs": [],
      "source": [
        "#importing our libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torch import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "!pip3 install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "\"\"\"## Loading our Heart Data\"\"\"\n",
        "\n",
        "data=pd.read_excel('/content/heart_disease.xlsx')\n",
        "\n",
        "data.drop(columns=['AgeCategory'],inplace=True)\n",
        "\n",
        "data.info()\n",
        "\n",
        "data.describe()\n",
        "\n",
        "data.describe(include='object')\n",
        "\n",
        "\"\"\"# Building a class to clean our ANN data\"\"\"\n",
        "\n",
        "class CleanHeart():\n",
        "  def __init__(self,datafile):\n",
        "    self.datafile = datafile  \n",
        "  def dum(self):\n",
        "    dummy=pd.get_dummies(self.datafile,columns=['Smoking',\t'AlcoholDrinking',\t'Stroke',\t'DiffWalking',\t'Sex',\n",
        "    'Race',\t'Diabetic',\t'PhysicalActivity',\t'GenHealth',\t'Asthma',\t'KidneyDisease','SkinCancer'],drop_first=False)\n",
        "    self.datafile.drop(self.datafile.columns[[1,2,3,4,7,8,9,10,11,12,13,14,15]],axis=1,inplace=True)\n",
        "    return pd.concat([dummy],axis=1)\n",
        "    dum(self.datafile)\n",
        "    print(self.datafile)\n",
        "\n",
        "\"\"\"#Calling our class and transforming it into a tensor for our model\"\"\"\n",
        "\n",
        "data2=CleanHeart(data)\n",
        "clean=data2.dum() #need to split x and y and turn y into 0,1 dummies\n",
        "clean\n",
        "\n",
        "clean['Heart_Dummy'] = clean.apply(lambda y: 1 if y['HeartDisease'] == 'Yes' else 0, axis=1) #one hot encoding applied here \n",
        "clean.drop(columns=['HeartDisease'],inplace=True)\n",
        "clean\n",
        "\n",
        "\"\"\"# in future consider using stepwise log regression for feature selection\"\"\"\n",
        "\n",
        "\"\"\"from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#creating a tensor fo our data\n",
        "\n",
        "clean_tensor = torch.tensor(clean.values)\n",
        "clean_tensor.shape\n",
        "\n",
        "#importing more packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "\"\"\"# Building our Neural Network Architecture\"\"\"\n",
        "\n",
        "#defining our nerual network architecture for training and test(based on values we will determine if we overfit our model and plan accordingly(early stopping or L1 / L2 REG))\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,n_features,Classes):\n",
        "    super(NeuralNet,self).__init__()# this is our subclass that inherites attributes from nn.Module\n",
        "    self.InputLayer=nn.Linear(n_features,38)\n",
        "    self.HiddenLayer=nn.Linear(38,38)\n",
        "    self.HiddenLayer2=nn.Linear(38,38)# second hidden layer\n",
        "    self.Outter=nn.Linear(38,1)\n",
        "    #layers of the network\n",
        "\n",
        "    #so the issue here is with in the layers inputs and outputs (inputs in layer 1 = n vars, input to hidden layer needs to be \n",
        "    #figured out as well as the output to the outter layer)\n",
        "\n",
        "  def forward(self,x): #this forward function specifies what will happen at each layer (computaion of our data)\n",
        "    x = F.relu(self.HiddenLayer(x)) #setting our activation function for our hidden layer\n",
        "    x = F.relu(self.HiddenLayer2(x))\n",
        "    x = F.logsigmoid(self.Outter(x)) #setting our activation function for our output layer\n",
        "    #x = self.Outter(x) #see if you need to define a loss funciton here \n",
        "    return x #returning the output layer values\n",
        "\n",
        "\"\"\"#Storing our model in either a GPU or CPU\"\"\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\"\"\"#HyperParamaters\"\"\"\n",
        "\n",
        "#applying the model\n",
        "#model = NeuralNet(n_features,Classes)\n",
        "\n",
        "#Setting our model hyperparameters.... in future try grid search method to find optimal params \n",
        "n_features= 38 #features in our dataset\n",
        "Classes = 2 #our binary outcome var\n",
        "Learning_rate = 0.1 \n",
        "Batch_Size = 200 #number of obs we want at each training epoch.... training time is long... find optimal batch size \n",
        "Epochs = 1 #number of time we want our data to pass through our neural network\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "model = NeuralNet(n_features=n_features,Classes=Classes).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr = Learning_rate)\n",
        "\n",
        "\n",
        "#using this class to transform our data to tensors and run a x and y split\n",
        "class HeartFailureData():  # this method is best when using custom datasets i.e data that is not offered by pytorch \n",
        "  def __init__(self):\n",
        "    self.x = clean_tensor.float()\n",
        "    self.y = clean_tensor[:,[37]]\n",
        "    self.n_samples=self.x.shape[0]\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "      return self.x[index],self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.n_samples #new dataset\n",
        "\n",
        "\n",
        "\"\"\"#Loading the data using DataLoader\"\"\"\n",
        "\n",
        "dataset=HeartFailureData() #using dataloader and dataset to load our data into our pytorch nerual net\n",
        "train_dataloader = DataLoader(dataset=dataset,batch_size=Batch_Size,shuffle=True) #issue here is that train_loader is only giving 46 obs when batch size > 1\n",
        "#loading the training data\n",
        "\n",
        "#figure out this train test split\n",
        "#train_dataset = datasets(train=True,download=False)\n",
        "#train,test=random_split(train_dataset[600,318]) #do a 70/30 split for the data 70 train, 30 test\n",
        "#transforming train dataloader to a dataset...do the same for test set as well \n",
        "\n",
        "#test_dataloader = DataLoader(dataset=dataset,batch_size=Batch_Size,shuffle=True)\n",
        "\n",
        "\"\"\"#Training our Model using Gradient Descent\"\"\"\n",
        "\n",
        "for epoch in torch.arange(Batch_Size): #pytorch will remove the use of the range function in future updates instead the new function will be torch.arange\n",
        "  for Batch in train_dataloader:\n",
        "    xval,targetvar=Batch\n",
        "    xval=xval.to(device=device)\n",
        "    targetvar=targetvar.to(device=device) \n",
        "   #stores both the x and y values in our cpu device\n",
        "    print(xval.shape) #is the shape based on n rows and n columns or n_dimention and n_columns?\n",
        "    \n",
        "    \n",
        "    #forward pass\n",
        "    acurcy_score = model(xval.float()) #try to employ early stopping here \n",
        "    loss = loss_func(acurcy_score,targetvar)\n",
        "\n",
        "    #backward propagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "   \n",
        "    optimizer.step()\n",
        "\n",
        "#checking model accuracy and determine overfitting \n",
        "\n",
        "def accuracy(train_dataloader,model):\n",
        "  accurate = 0\n",
        "  num_sample = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X_train,y_train in train_dataloader:\n",
        "      X_train=X_train.to(device=device)\n",
        "      y_train=y_train.to(device=device)\n",
        "      \n",
        "      acurcy_score = model(X_train)\n",
        "      _,pred = acurcy_score.max(0)\n",
        "      accurate +=(pred == y_train).sum()\n",
        "      num_sample += pred.size(0)\n",
        "      print(f'got {accurate}/{num_sample} with accuracy rate of ,{float(accurate)/float(num_sample)*100:.2f} at epoch {Epochs}')\n",
        "    model.train()\n",
        "\n",
        "    # in future implement early stopping for the network to prevent model overfitting\n",
        "    \n",
        "#checking the accuracy at each epoch\n",
        "accuracy(train_dataloader,model)"
      ]
    }
  ]
}